{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "voting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdbaKn3pN_SU"
      },
      "source": [
        "## Ensemble methods: Voting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMNzBmuSbkzd"
      },
      "source": [
        "Hi! Here we are going to explore how the voting classifier works, and compare them to the base classifiers we chose below. Voting classifiers work by aggregating the predictions of each classifier, and then predicting the class that gets the most votes. This method is called hard voting, but this classifier will be using soft voting instead. This will make it so that it will predict the class with the highest probabilty, which till give more weight to highly confident votes, rather than just majority vote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKICMtXOOOge"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7rnXh_3yFYF"
      },
      "source": [
        "The data that we wil be using is from Scikit-Learn's dataset library called digits. The dataset is made up of 1797 8x8 images. Each image represents a hand-written digit that range from 0 - 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnaVoXSqTEGz"
      },
      "source": [
        "mnist = datasets.load_digits()\n",
        "X, y = mnist.data, mnist.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys0i9Wjn2zfo"
      },
      "source": [
        "Lets take a look at one of these images at a random index of the data. We would first have to transform it into an 8x8 feature vector. Each feature represents one pixel's intensity, ranging from 0 - 255 (white to black)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQV3kRJfT3Nx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "27cbc88e-b85c-4adf-b2f2-8999fbf9d28a"
      },
      "source": [
        "index = np.random.randint(len(X))\n",
        "some_digit = X[index]\n",
        "some_digit_image = some_digit.reshape(8, 8)\n",
        "\n",
        "plt.imshow(some_digit_image, cmap=\"binary\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADzElEQVR4nO3dQVFjQRhG0Zcp9kRCUAAOsIADLMQBEggSUEAcgIRIwAGgILOaHcXs/tzJnLPkLb6miltdxaZXx+NxAXp+nfoAwPfECVHihChxQpQ4IeriL9/P8l+52+12dO/p6Wls6/LycmxrvV6PbR0Oh7GtZZn93ZZlWX33QzcnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTolZ/eTx37DmGz8/Pqalls9mMbU17fn4e23p5eRnburm5GdtalvEnOzzHAP8ScUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUxakP8Md+vx/b+vr6GttalmX5+PgY21qv12Nbt7e3Y1u73W5sq8LNCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQlTmrZT39/exrevr67GtZZl9v2TS4XA49RHOmpsTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUZnnGCafLNhut2Nb52zyCY3NZjO2VeHmhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQlTmOYarq6uxrdfX17Gtc7bb7ca23t7exrYq3JwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IWh2Px5++//jxX7VarUb39vv92Nbj4+PY1qQzf47h2z9INydEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRF6c+wCk8PDyM7t3f349t3d3djW3tdruxrf+RmxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRq+PxeOozAN9wc0KUOCFKnBAlTogSJ0SJE6J+A096Ri/1f+I1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdlSrV6sXiAR"
      },
      "source": [
        "Let's see the result of this image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmuZCe19UWO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48326e98-f647-43a1-ead7-4a57ee01f555"
      },
      "source": [
        "y[index]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y8XfEerZIVR"
      },
      "source": [
        "In order to build our base models, let's first split the data into train and test sets, and then scale it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sqyYilOZbu1"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# scale data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDGSXGIXs25x"
      },
      "source": [
        "Here we have 3 base classifiers, Logisitic Regression, Random Forest, and Support Vector Machine. Lets compare the accuracy between our base classifier and then see our voting classifier's accuracy. As we can see, our base classifier's accuracy is pretty high, and our voting classifier is slighty better than the SVM classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orKw-nqYrqQb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "eea1d04e-b84e-4631-971d-84db3d1c0dc7"
      },
      "source": [
        "log_classifier = LogisticRegression()\n",
        "random_classifier = RandomForestClassifier()\n",
        "svm_classifier = SVC(probability=True)\n",
        "\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[('lr', log_classifier), ('rf', random_classifier), ('svc', svm_classifier)],\n",
        "    voting='soft')\n",
        "\n",
        "for clf in (log_classifier, random_classifier, svm_classifier, voting_classifier):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, ':{:.4f}'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression :0.9593\n",
            "RandomForestClassifier :0.9759\n",
            "SVC :0.9852\n",
            "VotingClassifier :0.9852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWSTKs4XuXFq"
      },
      "source": [
        "Since our base classifier's were diverse, ensemble methods work best when predictors are independent from one another. Our base algorithms are different, they will make different kinds of errors, which would increase our the ensemble method's accuracy."
      ]
    }
  ]
}